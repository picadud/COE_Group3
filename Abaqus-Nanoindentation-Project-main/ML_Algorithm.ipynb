{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392e2478-1d19-4938-8c0f-5f78fc8aa1ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/lib/python3.10/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3.66071429e+02, 8.25000000e-01, 1.06250000e+04, 7.23214286e+02])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Reload the data files from specified sheets\n",
    "parameters_df = pd.read_excel('parameters_finaleee.xlsx', sheet_name='QP1200_2')\n",
    "target_df = pd.read_excel('parameters_final.xlsx', sheet_name='theory_QP1200_2')\n",
    "\n",
    "# Extract feature columns and target columns from the training data\n",
    "features = parameters_df[['tau0', 'a', 'h0', 'tcs']]\n",
    "targets = parameters_df[['peak', 'drop point', '25% peak', '25% drop', '50% peak', '50% drop', '75% peak', '75% drop']]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features for optimal performance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the multi-output regressor with Gradient Boosting as the base model\n",
    "model = MultiOutputRegressor(GradientBoostingRegressor())\n",
    "\n",
    "# Train the model on the scaled training data\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set and calculate the Mean Absolute Error (MAE) for performance\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Define the target outcomes based on the \"theory\" sheet data\n",
    "target_outcomes = target_df.mean()  # Adjust as needed to align with actual theoretical target\n",
    "\n",
    "tolerance = 0.05  # 5% tolerance\n",
    "\n",
    "# Function to calculate if the predictions are within the tolerance range\n",
    "def is_within_tolerance(predictions, targets, tolerance):\n",
    "    # Check if all predictions are within (1 ± tolerance) range of the target outcomes\n",
    "    lower_bound = targets * (1 - tolerance)\n",
    "    upper_bound = targets * (1 + tolerance)\n",
    "    return ((predictions >= lower_bound) & (predictions <= upper_bound)).all()\n",
    "\n",
    "# Start with an initial set of parameters (mean of training data as a reasonable starting point)\n",
    "current_params = X_train.mean().values.reshape(1, -1)\n",
    "scaled_current_params = scaler.transform(current_params)  # Scale initial parameters\n",
    "\n",
    "# Iterative optimization loop\n",
    "max_iterations = 1000\n",
    "for iteration in range(max_iterations):\n",
    "    # Predict the outcomes for the current parameters\n",
    "    predictions = model.predict(scaled_current_params).flatten()\n",
    "    \n",
    "    # Check if predictions are within tolerance\n",
    "    if is_within_tolerance(predictions, target_outcomes.values, tolerance):\n",
    "        optimal_params = scaler.inverse_transform(scaled_current_params)[0]  # Rescale to original parameters\n",
    "        break\n",
    "    \n",
    "    # Adjust each parameter slightly and check if it improves prediction toward target\n",
    "    improved = False\n",
    "    for param_index in range(current_params.shape[1]):\n",
    "        # Create a slight positive and negative adjustment to the current parameter\n",
    "        for adjustment in [-0.01, 0.01]:  # Small adjustments\n",
    "            test_params = scaled_current_params.copy()\n",
    "            test_params[0, param_index] += adjustment\n",
    "            test_predictions = model.predict(test_params).flatten()\n",
    "            \n",
    "            # Check if this adjustment brings predictions closer to the target\n",
    "            if is_within_tolerance(test_predictions, target_outcomes.values, tolerance):\n",
    "                optimal_params = scaler.inverse_transform(test_params)[0]\n",
    "                improved = True\n",
    "                break\n",
    "        if improved:\n",
    "            break\n",
    "    else:\n",
    "        # No improvement found with current adjustments; stop iterating\n",
    "        optimal_params = scaler.inverse_transform(scaled_current_params)[0]\n",
    "        break\n",
    "\n",
    "# Display the final optimal parameters found\n",
    "optimal_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada5f146-33f0-4a06-b030-15fa096d15f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 15:32:08.106509: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0411\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0290\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0189\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0126\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0078\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0049\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0030\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0023\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0019\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0018\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0018\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0016\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0015\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0012\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0010\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.0882e-04\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 6.6306e-04\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.2981e-04\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.6160e-04\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0052e-04\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.6768e-04\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.3022e-04\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.0472e-04\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7506e-04\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5067e-04\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.2422e-04\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.9746e-04\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.7789e-04\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6003e-04\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.4950e-04\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3957e-04\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.3191e-04\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2458e-04\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1770e-04\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0901e-04\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.0194e-04\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 9.4725e-05\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.8903e-05\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.4132e-05\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 8.0792e-05\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.7516e-05\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.4226e-05\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 7.1244e-05\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.8673e-05\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.6566e-05\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.3616e-05\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 6.1402e-05\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.9110e-05\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.6907e-05\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.5381e-05\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.3312e-05\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 5.1647e-05\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.9846e-05\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.8330e-05\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.6891e-05\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.5407e-05\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.3889e-05\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.2587e-05\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.1613e-05\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 4.0457e-05\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.9604e-05\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.8374e-05\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.7320e-05\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.6336e-05\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.5876e-05\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.4771e-05\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.3932e-05\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.2877e-05\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.2480e-05\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1763e-05\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.1275e-05\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0445e-05\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9833e-05\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.9126e-05\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8723e-05\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8082e-05\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.7463e-05\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7195e-05\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6682e-05\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6353e-05\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5818e-05\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.5397e-05\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4947e-05\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4325e-05\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4147e-05\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3530e-05\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.3332e-05\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.2831e-05\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.2749e-05\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.2167e-05\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.1744e-05\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.1566e-05\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.1629e-05\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.1149e-05\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.0642e-05\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.0412e-05\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.0094e-05\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.9928e-05\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.9839e-05\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.9555e-05\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "Test MAE: 0.010\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/lib/python3.10/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "No improvement found after iteration 0.\n",
      "Optimal Parameters Found: [3.66071429e+02 8.25000000e-01 1.06250000e+04 7.23214286e+02]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# --------------------------\n",
    "# Data Loading and Preparation\n",
    "# --------------------------\n",
    "parameters_df = pd.read_excel('parameters_finaleee.xlsx', sheet_name='QP1200_2')\n",
    "target_df = pd.read_excel('parameters_final.xlsx', sheet_name='theory_QP1200_2')\n",
    "\n",
    "# Extract feature columns and target columns\n",
    "features = parameters_df[['tau0', 'a', 'h0', 'tcs']]\n",
    "targets = parameters_df[['peak', 'drop point', '25% peak', '25% drop', '50% peak', '50% drop', '75% peak', '75% drop']]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features for optimal ANN performance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --------------------------\n",
    "# Build and Train the ANN Model\n",
    "# --------------------------\n",
    "# Define the neural network architecture\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(y_train.shape[1])  # Output layer with one neuron per target\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')  # Using MSE as a loss function\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train.values, epochs=100, batch_size=16, verbose=1)\n",
    "\n",
    "# Evaluate model performance on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Test MAE: {test_mae:.3f}\")\n",
    "\n",
    "# --------------------------\n",
    "# Parameter Targeting Logic\n",
    "# --------------------------\n",
    "# Define the target outcomes based on the \"theory\" sheet data\n",
    "# You may need to adjust how you define the target_outcomes.\n",
    "target_outcomes = target_df.mean()  # Using the mean of theory data as the target\n",
    "\n",
    "tolerance = 0.05  # 5% tolerance\n",
    "\n",
    "def is_within_tolerance(predictions, targets, tolerance):\n",
    "    # Check if all predictions are within (1 ± tolerance) of the target outcomes\n",
    "    lower_bound = targets * (1 - tolerance)\n",
    "    upper_bound = targets * (1 + tolerance)\n",
    "    return ((predictions >= lower_bound) & (predictions <= upper_bound)).all()\n",
    "\n",
    "# Start with an initial set of parameters (mean of training data as starting point)\n",
    "current_params = X_train.mean().values.reshape(1, -1)\n",
    "scaled_current_params = scaler.transform(current_params)  # Scale initial parameters\n",
    "\n",
    "# Iterative optimization loop\n",
    "max_iterations = 1000\n",
    "optimal_params = None\n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    # Predict outcomes for current parameters\n",
    "    predictions = model.predict(scaled_current_params).flatten()\n",
    "    \n",
    "    # Check tolerance\n",
    "    if is_within_tolerance(predictions, target_outcomes.values, tolerance):\n",
    "        optimal_params = scaler.inverse_transform(scaled_current_params)[0]\n",
    "        print(f\"Found parameters within tolerance at iteration {iteration}: {optimal_params}\")\n",
    "        break\n",
    "    \n",
    "    # Attempt to improve parameters\n",
    "    improved = False\n",
    "    for param_index in range(current_params.shape[1]):\n",
    "        # Create slight positive and negative adjustments\n",
    "        for adjustment in [-0.01, 0.01]:\n",
    "            test_params = scaled_current_params.copy()\n",
    "            test_params[0, param_index] += adjustment\n",
    "            \n",
    "            test_predictions = model.predict(test_params).flatten()\n",
    "            if is_within_tolerance(test_predictions, target_outcomes.values, tolerance):\n",
    "                optimal_params = scaler.inverse_transform(test_params)[0]\n",
    "                print(f\"Found parameters within tolerance after adjustment at iteration {iteration}: {optimal_params}\")\n",
    "                improved = True\n",
    "                break\n",
    "        if improved:\n",
    "            break\n",
    "    else:\n",
    "        # If no improvement found, break out\n",
    "        optimal_params = scaler.inverse_transform(scaled_current_params)[0]\n",
    "        print(f\"No improvement found after iteration {iteration}.\")\n",
    "        break\n",
    "\n",
    "if optimal_params is not None:\n",
    "    print(\"Optimal Parameters Found:\", optimal_params)\n",
    "else:\n",
    "    print(\"No parameters found within tolerance.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b021abb-c695-4494-a698-a30f25dd6ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
